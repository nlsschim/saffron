{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98887451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from saffron.io import data_io\n",
    "from saffron.data import datasets, data_processing\n",
    "from saffron.models import torch_models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7566897",
   "metadata": {},
   "outputs": [],
   "source": [
    "microglia_im_path = \"/Volumes/imagereg/Cross_Species_Study/mice/p12/denoised_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a3ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = data_io.load_images_from_directory(microglia_im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4387b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pipeline Component 1: Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_grayscale(image: np.ndarray, channel: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert image to 2D grayscale regardless of input dimensions.\n",
    "    \n",
    "    Handles:\n",
    "    - 2D: (H, W) -> return as-is\n",
    "    - 3D: (C, H, W) or (H, W, C) -> extract channel\n",
    "    - 4D: (C, Z, H, W) or (Z, H, W, C) -> max projection + extract channel\n",
    "    \"\"\"\n",
    "    # Case 1: Already 2D grayscale\n",
    "    if image.ndim == 2:\n",
    "        return image\n",
    "    \n",
    "    # Case 2: 3D with channels first (C, H, W)\n",
    "    elif image.ndim == 3:\n",
    "        if image.shape[0] in [1, 3, 4] and image.shape[0] < image.shape[1]:\n",
    "            return image[channel]\n",
    "        # Channels last (H, W, C)\n",
    "        elif image.shape[2] in [1, 3, 4] and image.shape[2] < image.shape[0]:\n",
    "            return image[:, :, channel]\n",
    "        # Z-stack without channel dimension (Z, H, W) - max project\n",
    "        else:\n",
    "            return np.max(image, axis=0)\n",
    "    \n",
    "    # Case 3: 4D with channels first (C, Z, H, W) - max project Z\n",
    "    elif image.ndim == 4:\n",
    "        if image.shape[0] in [1, 3, 4] and image.shape[0] < image.shape[1]:\n",
    "            max_proj = np.max(image[channel], axis=0)\n",
    "            return max_proj\n",
    "        # Channels last (Z, H, W, C) - max project Z\n",
    "        elif image.shape[3] in [1, 3, 4] and image.shape[3] < image.shape[0]:\n",
    "            max_proj = np.max(image[:, :, :, channel], axis=0)\n",
    "            return max_proj\n",
    "    \n",
    "    # Unsupported shape\n",
    "    raise ValueError(f\"Cannot determine channel dimension for shape {image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e35b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_quadrants(image: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    Split a 2D image into 4 quadrants.\n",
    "    \n",
    "    Returns list of dicts with keys: 'image', 'quadrant', 'position'\n",
    "    Quadrants: 'TL' (top-left), 'TR' (top-right), 'BL' (bottom-left), 'BR' (bottom-right)\n",
    "    \"\"\"\n",
    "    if image.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D image, got shape {image.shape}\")\n",
    "    \n",
    "    h, w = image.shape\n",
    "    mid_h = h // 2\n",
    "    mid_w = w // 2\n",
    "    \n",
    "    quadrants = []\n",
    "    \n",
    "    # Top-left\n",
    "    quadrants.append({\n",
    "        'image': image[:mid_h, :mid_w],\n",
    "        'quadrant': 'TL',\n",
    "        'position': (0, 0)\n",
    "    })\n",
    "    \n",
    "    # Top-right\n",
    "    quadrants.append({\n",
    "        'image': image[:mid_h, mid_w:],\n",
    "        'quadrant': 'TR',\n",
    "        'position': (0, mid_w)\n",
    "    })\n",
    "    \n",
    "    # Bottom-left\n",
    "    quadrants.append({\n",
    "        'image': image[mid_h:, :mid_w],\n",
    "        'quadrant': 'BL',\n",
    "        'position': (mid_h, 0)\n",
    "    })\n",
    "    \n",
    "    # Bottom-right\n",
    "    quadrants.append({\n",
    "        'image': image[mid_h:, mid_w:],\n",
    "        'quadrant': 'BR',\n",
    "        'position': (mid_h, mid_w)\n",
    "    })\n",
    "    \n",
    "    return quadrants\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68380fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_float32(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize image to float32 in range [0, 1].\n",
    "    \n",
    "    Handles uint8, uint16, and float inputs.\n",
    "    \"\"\"\n",
    "    # Convert to float32\n",
    "    image_float = image.astype(np.float32)\n",
    "    \n",
    "    # Min-max normalization to [0, 1]\n",
    "    img_min = image_float.min()\n",
    "    img_max = image_float.max()\n",
    "    \n",
    "    if img_max - img_min == 0:\n",
    "        return np.zeros_like(image_float)\n",
    "    \n",
    "    normalized = (image_float - img_min) / (img_max - img_min)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9437e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image: np.ndarray, target_size: tuple = (512, 512)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resize image to target size.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Input 2D image\n",
    "    target_size : tuple\n",
    "        Target (height, width)\n",
    "    \"\"\"\n",
    "    if image.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D image, got shape {image.shape}\")\n",
    "    \n",
    "    # Already correct size\n",
    "    if image.shape == target_size:\n",
    "        return image\n",
    "    \n",
    "    # Resize using bilinear interpolation\n",
    "    # cv2.resize expects (width, height) not (height, width)\n",
    "    resized = cv2.resize(image, (target_size[1], target_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clahe(image: np.ndarray, clip_limit: float = 2.0, tile_grid_size: tuple = (8, 8)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply Contrast Limited Adaptive Histogram Equalization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Input image (should be in 0-1 range, float32)\n",
    "    clip_limit : float\n",
    "        Threshold for contrast limiting\n",
    "    tile_grid_size : tuple\n",
    "        Size of grid for histogram equalization\n",
    "    \"\"\"\n",
    "    # CLAHE requires uint8, so convert from [0, 1] float to [0, 255] uint8\n",
    "    image_uint8 = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Create CLAHE object\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    \n",
    "    # Apply CLAHE\n",
    "    enhanced = clahe.apply(image_uint8)\n",
    "    \n",
    "    # Convert back to float32 [0, 1]\n",
    "    enhanced_float = enhanced.astype(np.float32) / 255.0\n",
    "    return enhanced_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eab193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image: np.ndarray, \n",
    "                     channel: int = 0,\n",
    "                     target_size: tuple = (512, 512),\n",
    "                     use_clahe: bool = True,\n",
    "                     split_quadrants: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for a single image.\n",
    "    \n",
    "    Returns list of dicts with keys: 'image', 'quadrant' (if split), 'position' (if split)\n",
    "    If not split, returns list with single dict containing 'image'.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert to grayscale 2D\n",
    "    gray = to_grayscale(image, channel=channel)\n",
    "    \n",
    "    # Step 2: Normalize to [0, 1]\n",
    "    normalized = normalize_to_float32(gray)\n",
    "    \n",
    "    # Step 3: Resize to target size\n",
    "    resized = resize_image(normalized, target_size=target_size)\n",
    "    \n",
    "    # Step 4: Apply CLAHE if requested\n",
    "    if use_clahe:\n",
    "        processed = apply_clahe(resized)\n",
    "    else:\n",
    "        processed = resized\n",
    "    \n",
    "    # Step 5: Split into quadrants if requested\n",
    "    if split_quadrants:\n",
    "        return split_into_quadrants(processed)\n",
    "    else:\n",
    "        return [{'image': processed, 'quadrant': None, 'position': None}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ims = []\n",
    "\n",
    "for im in images:\n",
    "\n",
    "    processed_ims.append(preprocess_image(im.data, channel=1, split_quadrants=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b441792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(processed_list: list, n_examples: int = 4, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Plot a few examples from preprocessed output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    processed_list : list\n",
    "        Output from preprocess_image()\n",
    "    n_examples : int\n",
    "        Number of examples to show (default: 4)\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    n_show = min(n_examples, len(processed_list))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_show, figsize=(4 * n_show, 4))\n",
    "    if n_show == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(n_show):\n",
    "        axes[i].imshow(processed_list[i]['image'], cmap='gray', vmin=0, vmax=1)\n",
    "        title = f\"{processed_list[i]['quadrant']}\" if processed_list[i]['quadrant'] else f\"Image {i}\"\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5aafd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(processed_ims[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ac53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_images(processed_list: list, output_dir: str, base_filename: str):\n",
    "    \"\"\"\n",
    "    Save each processed image/quadrant as a separate .npy file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    processed_list : list\n",
    "        Output from preprocess_image()\n",
    "    output_dir : str\n",
    "        Directory to save files\n",
    "    base_filename : str\n",
    "        Base name for files (e.g., 'image_01')\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    for item in processed_list:\n",
    "        # Create filename\n",
    "        if item['quadrant']:\n",
    "            filename = f\"{base_filename}_{item['quadrant']}.npy\"\n",
    "        else:\n",
    "            filename = f\"{base_filename}.npy\"\n",
    "        \n",
    "        filepath = output_path / filename\n",
    "        \n",
    "        # Save as .npy\n",
    "        np.save(filepath, item['image'])\n",
    "        saved_files.append(str(filepath))\n",
    "    \n",
    "    print(f\"Saved {len(saved_files)} files to {output_dir}\")\n",
    "    return saved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_directory(input_dir: str, \n",
    "                         output_dir: str,\n",
    "                         channel: int = 0,\n",
    "                         target_size: tuple = (512, 512),\n",
    "                         use_clahe: bool = True,\n",
    "                         split_quadrants: bool = False,\n",
    "                         file_extensions: list = ['.tif', '.tiff']) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess all images in a directory and save results.\n",
    "    \n",
    "    Returns list of all saved file paths.\n",
    "    \"\"\"\n",
    "    import tifffile\n",
    "    \n",
    "    input_path = Path(input_dir)\n",
    "    all_saved_files = []\n",
    "    \n",
    "    # Find all image files\n",
    "    image_files = []\n",
    "    for ext in file_extensions:\n",
    "        image_files.extend(input_path.glob(f\"*{ext}\"))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_file in image_files:\n",
    "        try:\n",
    "            # Load image\n",
    "            image = tifffile.imread(str(img_file))\n",
    "            \n",
    "            # Get base filename (without extension)\n",
    "            base_name = img_file.stem\n",
    "            \n",
    "            # Preprocess\n",
    "            result = preprocess_image(image, channel, target_size, use_clahe, split_quadrants)\n",
    "            \n",
    "            # Save results\n",
    "            saved = save_processed_images(result, output_dir, base_name)\n",
    "            all_saved_files.extend(saved)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nProcessed {len(image_files)} images -> {len(all_saved_files)} output files\")\n",
    "    return all_saved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_directory(\"/Volumes/imagereg/Cross_Species_Study/pig/p1/denoised_all\",\n",
    "                     \"/Volumes/imagereg/Cross_Species_Study/pig/p1/denoised_all/preprocessed\", channel=1, split_quadrants=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd260c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saffron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
